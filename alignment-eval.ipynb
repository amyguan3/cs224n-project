{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (WhisperFeatureExtractor, \n",
    "                          WhisperTokenizer, \n",
    "                          WhisperProcessor,\n",
    "                          WhisperModel,\n",
    "                          WhisperForConditionalGeneration, \n",
    "                          Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, \n",
    "                          TrainerCallback, \n",
    "                          TrainingArguments, \n",
    "                          TrainerState, \n",
    "                          TrainerControl,\n",
    "                          pipeline)\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from peft import (prepare_model_for_int8_training,\n",
    "                  LoraConfig, \n",
    "                  PeftModel, \n",
    "                  LoraModel, \n",
    "                  LoraConfig, \n",
    "                  TaskType,\n",
    "                  get_peft_model)\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.utils import check_min_version\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from trainer_utils import AlignmentSeq2SeqTrainer\n",
    "from data_utils import (DataCollatorSpeechSeq2SeqWithPadding, \n",
    "                        load_sd_qa_dataset, \n",
    "                        filter_data)\n",
    "from eval_utils import (evaluate_asr,\n",
    "                        get_mini_cv)\n",
    "import csv\n",
    "import pickle\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "!pip install -q transformers librosa datasets==2.14.6 evaluate jiwer gradio bitsandbytes==0.37 accelerate geomloss gradio torchaudio\n",
    "!pip install -q git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SavePeftCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "\n",
    "        # record the losses\n",
    "        loss_file = os.path.join(args.output_dir, 'loss.csv')\n",
    "        with open(loss_file, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([state.global_step, state.log_history[\"loss\"][-1]]) # iter, loss\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# log in to huggingface to save model as you go\n",
    "# notebook_login()\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load whisper feature extractor, tokenizer, processor\n",
    "model_path = \"openai/whisper-base\"\n",
    "task = \"transcribe\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_path, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa3dbbf345a4900ae8f08d194a2fac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8125871d12bc4336a46b87c542ec2379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6502c9ec7440e79ae5a08ea6d9e205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # load pre-trained model checkpoint\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "# model.hf_device_map = {\" \":0}  # not super sure what to map to here\n",
    "model.config.forced_decoder_ids = None  # no tokens forced for decoder outputs\n",
    "model.config.suppress_tokens = []\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668c83c930694a28add47f757c709a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/810 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb78fd894cfc4aacbd9b733b7f817afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bfba58054741749772f4dcd524a044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/366M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe74259b8c2477594b297789562c044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b9f207705d4cd9a34e51ae923980a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/373M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83cac211f2b4a4c8de6af3453d2f8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/382M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7fb52d2b18421c92ab2f86a49ee590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f23d2889546308c82802ac15e2ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050ac4169b754bf09228376fc7a3c26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4847f51abb4372a6bdc34db294da24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45ef1b755f4309abec510f3731d784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '-1008642825401516622', 'ind_n': {'path': None, 'array': array([ 0.00000000e+00, -3.05175781e-05, -3.05175781e-05, ...,\n",
      "        3.96728516e-04,  2.13623047e-04,  6.10351562e-05]), 'sampling_rate': 16000}, 'usa': {'path': None, 'array': array([0.        , 0.        , 0.        , ..., 0.00201416, 0.00259399,\n",
      "       0.00262451]), 'sampling_rate': 16000}}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "target_dialect = 'usa'\n",
    "source_dialect = 'ind_n'\n",
    "sd_qa = filter_data(load_sd_qa_dataset(), source=source_dialect, target=target_dialect)\n",
    "\n",
    "print(sd_qa['dev'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def prepare_source_data(data):\n",
    "    # compute log-Mel input features from audio arrays\n",
    "    data[\"source_input_features\"] = feature_extractor(data[source_dialect][\"array\"], sampling_rate=data[source_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    data[\"target_input_features\"] = feature_extractor(data[target_dialect][\"array\"], sampling_rate=data[target_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to gpu\n",
    "# run everything at once -> no for loop\n",
    "def prepare_target_embeddings(data):\n",
    "    # compute log-Mel input features from target audio array\n",
    "    # batch_size = 128\n",
    "    target_embeddings = []\n",
    "    decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "    # for i in range(0, len(data[\"target_input_features\"]), batch_size):\n",
    "    input_features = torch.tensor(data[\"target_input_features\"]).unsqueeze(0).to(device)\n",
    "    print(input_features.shape)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_features, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.encoder_hidden_states[-1]\n",
    "    target_embeddings = [embedding for embedding in last_hidden_state]\n",
    "    data[\"target_embeddings\"] = target_embeddings\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_qa = sd_qa.map(prepare_source_data, desc=\"Extract features for source dialect\"\n",
    "                    ).map(prepare_target_embeddings, desc=\"Original hidden embeddings for target dialect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------LORA PART------------\n",
    "target_modules = ['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2']\n",
    "config = LoraConfig(r=32, # rank, adjust this\n",
    "                lora_alpha=64, \n",
    "                target_modules = target_modules, \n",
    "                lora_dropout=0.05, \n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.FEATURE_EXTRACTION,  # check this???\n",
    "                )  \n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"azure-224n/test\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AlignmentSeq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=sd_qa['dev'],\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "peft_model_id = \"azure-224n/whisper-base-alignment\"\n",
    "model.push_to_hub(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id) # attaches the PEFT module to the Whisper model\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_mini_cv()\n",
    "metrics = evaluate_asr(model, processor, dataset, True)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
