
  0%|                                                                                                                                   | 0/480 [00:00<?, ?it/s]/opt/conda/envs/cs224n-project-env/lib/python3.9/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/cs224n-project-env/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...


































































































 21%|████████████████████████▊                                                                                              | 100/480 [49:46<3:09:52, 29.98s/it]
{'loss': 84.3232, 'learning_rate': 0.0008930232558139535, 'epoch': 3.12}

































































































 41%|████████████████████████████████████████████████▌                                                                    | 199/480 [1:41:05<2:22:19, 30.39s/it]


 42%|████████████████████████████████████████████████▊                                                                    | 200/480 [1:42:50<2:21:45, 30.38s/it]


































































































 62%|████████████████████████████████████████████████████████████████████████▉                                            | 299/480 [2:31:58<1:32:07, 30.54s/it]

 62%|█████████████████████████████████████████████████████████████████████████▏                                           | 300/480 [2:32:28<1:31:22, 30.46s/it]



































































































 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 400/480 [3:23:54<40:32, 30.40s/it]
{'loss': 41.0162, 'learning_rate': 0.00019534883720930232, 'epoch': 12.5}














































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 480/480 [4:04:21<00:00, 30.54s/it]
{'train_runtime': 14667.1611, 'train_samples_per_second': 1.023, 'train_steps_per_second': 0.033, 'train_loss': 53.78583017985026, 'epoch': 15.0}
Done with training! Pushing to hub...
asyzhou/224n-whisper-small-overnight-n_ind-fr

adapter_model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 52.0M/52.0M [00:01<00:00, 33.2MB/s]
Running training process 1 ...
Hyperparameters are (0.001, 32, 64, 100)
Load 8bit model...
Start training...
/opt/conda/envs/cs224n-project-env/lib/python3.9/site-packages/peft/utils/other.py:145: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 25,952,256 || all params: 267,687,168 || trainable%: 9.694994419754929



































































































 21%|████████████████████████▊                                                                                              | 100/480 [50:13<3:10:30, 30.08s/it]
{'loss': 82.7042, 'learning_rate': 0.0008906976744186047, 'epoch': 3.12}




































































































 41%|████████████████████████████████████████████████▌                                                                    | 199/480 [1:42:15<2:23:10, 30.57s/it]

 42%|████████████████████████████████████████████████▊                                                                    | 200/480 [1:42:45<2:22:57, 30.63s/it]

































































































 62%|████████████████████████████████████████████████████████████████████████▉                                            | 299/480 [2:33:09<1:31:37, 30.37s/it]


 62%|█████████████████████████████████████████████████████████████████████████▏                                           | 300/480 [2:35:21<1:30:55, 30.31s/it]

































































































 83%|██████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 399/480 [3:24:36<41:25, 30.68s/it]

 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 400/480 [3:25:06<40:36, 30.46s/it]
















































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 480/480 [4:05:48<00:00, 30.73s/it]
{'train_runtime': 14748.6175, 'train_samples_per_second': 1.017, 'train_steps_per_second': 0.033, 'train_loss': 52.73322499593099, 'epoch': 15.0}
Done with training! Pushing to hub...
asyzhou/224n-whisper-small-overnight-n_ind-fr
README.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.18k/5.18k [00:00<00:00, 1.06MB/s]

adapter_model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 104M/104M [00:02<00:00, 38.4MB/s]
Running training process 2 ...
Hyperparameters are (0.001, 32, 128, 100)
Load 8bit model...
Start training...
trainable params: 51,904,512 || all params: 293,639,424 || trainable%: 17.676274967764545


































































































 21%|████████████████████████▊                                                                                              | 100/480 [50:05<3:11:32, 30.24s/it]

 21%|████████████████████████▊                                                                                              | 100/480 [51:48<3:11:32, 30.24s/it]


































































































 41%|████████████████████████████████████████████████▌                                                                    | 199/480 [1:40:55<2:22:44, 30.48s/it]


 42%|████████████████████████████████████████████████▊                                                                    | 200/480 [1:43:10<2:21:57, 30.42s/it]


































































































 62%|█████████████████████████████████████████████████████████████████████████▏                                           | 300/480 [2:32:50<1:31:11, 30.40s/it]

 62%|█████████████████████████████████████████████████████████████████████████▏                                           | 300/480 [2:34:34<1:31:11, 30.40s/it]




































































































 83%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 400/480 [3:24:23<40:40, 30.51s/it]
{'loss': 40.2954, 'learning_rate': 0.00019069767441860466, 'epoch': 12.5}
















































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 480/480 [4:04:39<00:00, 30.58s/it]
{'train_runtime': 14679.5911, 'train_samples_per_second': 1.022, 'train_steps_per_second': 0.033, 'train_loss': 52.785134379069014, 'epoch': 15.0}
Done with training! Pushing to hub...
asyzhou/224n-whisper-small-overnight-n_ind-fr



adapter_model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 208M/208M [00:05<00:00, 34.8MB/s]
Running training process 3 ...
Hyperparameters are (0.001, 64, 32, 50)
Load 8bit model...
Start training...
trainable params: 12,976,128 || all params: 254,711,040 || trainable%: 5.094450558562362




























