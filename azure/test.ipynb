{"cells":[{"cell_type":"markdown","metadata":{},"source":["Code modified from https://github.com/Vaibhavs10/fast-whisper-finetuning"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare environment"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n","%pip install -q git+https://github.com/huggingface/peft.git@main"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf168a44ad3f45289607080593042f8f","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login() #huggingface-cli login workaround"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Applications/miniconda3/envs/local_nmt/lib/python3.8/site-packages/datasets/load.py:1454: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n","Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.18k/8.18k [00:00<00:00, 10.1MB/s]\n","Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.7k/14.7k [00:00<00:00, 9.17MB/s]\n","Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.65k/3.65k [00:00<00:00, 7.12MB/s]\n","Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.4k/65.4k [00:00<00:00, 370kB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.2k/13.2k [00:00<00:00, 10.6MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118M/118M [00:18<00:00, 6.42MB/s] \n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.4M/65.4M [00:05<00:00, 11.5MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94.2M/94.2M [00:09<00:00, 10.3MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119M/119M [00:09<00:00, 12.1MB/s] \n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2M/24.2M [00:04<00:00, 5.65MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34M/1.34M [00:00<00:00, 9.37MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661k/661k [00:00<00:00, 3.81MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844k/844k [00:00<00:00, 9.08MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.09M/1.09M [00:00<00:00, 3.57MB/s]\n","Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210k/210k [00:00<00:00, 902kB/s]  \n","Reading metadata...: 4479it [00:00, 144181.61it/s]les/s]\n","Generating train split: 4479 examples [00:00, 11160.58 examples/s]\n","Reading metadata...: 2281it [00:00, 223147.07it/s]examples/s]\n","Generating validation split: 2281 examples [00:00, 10969.04 examples/s]\n","Reading metadata...: 2947it [00:00, 240553.75it/s]es/s]\n","Generating test split: 2947 examples [00:00, 10995.52 examples/s]\n","Reading metadata...: 3487it [00:00, 249229.56it/s]les/s]\n","Generating other split: 3487 examples [00:00, 10245.49 examples/s]\n","Reading metadata...: 706it [00:00, 255340.06it/s]? examples/s]\n","Generating invalidated split: 706 examples [00:00, 10257.15 examples/s]\n","/Applications/miniconda3/envs/local_nmt/lib/python3.8/site-packages/datasets/load.py:2508: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n","You can remove this warning by passing 'token=<use_auth_token>' instead.\n","  warnings.warn(\n","/Applications/miniconda3/envs/local_nmt/lib/python3.8/site-packages/datasets/load.py:1454: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n","        num_rows: 6760\n","    })\n","    test: Dataset({\n","        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n","        num_rows: 2947\n","    })\n","})\n"]}],"source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()\n","\n","language_abbr = \"hi\" # Replace with the language ID of your choice here!\n","\n","common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", language_abbr, split=\"train+validation\", use_auth_token=True)\n","common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", language_abbr, split=\"test\", use_auth_token=True)\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 6760\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 2947\n","    })\n","})\n"]}],"source":["common_voice = common_voice.remove_columns(\n","    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",")\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 185k/185k [00:00<00:00, 3.77MB/s]\n"]}],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 805/805 [00:00<00:00, 529kB/s]\n","vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 836k/836k [00:00<00:00, 12.6MB/s]\n","tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.48M/2.48M [00:00<00:00, 16.0MB/s]\n","merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 494k/494k [00:00<00:00, 6.23MB/s]\n","normalizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52.7k/52.7k [00:00<00:00, 31.0MB/s]\n","added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.6k/34.6k [00:00<00:00, 17.7MB/s]\n","special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.08k/2.08k [00:00<00:00, 2.41MB/s]\n"]}],"source":["from transformers import WhisperTokenizer\n","\n","task = \"transcribe\"\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=language_abbr, task=task)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=language_abbr, task=task)"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare data"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'audio': {'path': '/Users/amyzhou/.cache/huggingface/datasets/downloads/extracted/434586742ec36ad34a876813d2a17bf0b9e979d6cb4f14cd22e11cf8cab61fdf/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 1.17937861e-25, -6.46234854e-27, -1.37324906e-26, ...,\n","        1.06425851e-07,  4.46414674e-08,  2.61410094e-09]), 'sampling_rate': 48000}, 'sentence': 'à¤¹à¤®à¤¨à¥‡ à¤‰à¤¸à¤•à¤¾ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤®à¤¨à¤¾à¤¯à¤¾à¥¤'}\n"]}],"source":["print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from datasets import Audio\n","\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'audio': {'path': '/Users/amyzhou/.cache/huggingface/datasets/downloads/extracted/434586742ec36ad34a876813d2a17bf0b9e979d6cb4f14cd22e11cf8cab61fdf/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([-6.93889390e-18, -1.90819582e-17, -3.46944695e-17, ...,\n","       -1.31141860e-07,  2.61934474e-07,  4.65079211e-08]), 'sampling_rate': 16000}, 'sentence': 'à¤¹à¤®à¤¨à¥‡ à¤‰à¤¸à¤•à¤¾ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤®à¤¨à¤¾à¤¯à¤¾à¥¤'}\n"]}],"source":["print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6760/6760 [00:38<00:00, 174.75 examples/s]\n","Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2947/2947 [00:17<00:00, 166.80 examples/s]\n"]}],"source":["common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_features', 'labels'],\n","    num_rows: 6760\n","})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["common_voice[\"train\"]"]},{"cell_type":"markdown","metadata":{},"source":["# Training and evaluation"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n","import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.49k/4.49k [00:00<00:00, 6.26MB/s]\n"]}],"source":["# Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric.\n","import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"ename":"ImportError","evalue":"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WhisperForConditionalGeneration\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/whisper-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/Applications/miniconda3/envs/local_nmt/lib/python3.8/site-packages/transformers/modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m/Applications/miniconda3/envs/local_nmt/lib/python3.8/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:62\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"]}],"source":["# Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n","from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", load_in_8bit=True, device_map=\"auto\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the training configuration: this will be used by the ðŸ¤— Trainer to define the training schedule.\n"]}],"metadata":{"kernelspec":{"display_name":"local_nmt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":2}
