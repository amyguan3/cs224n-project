{"cells":[{"cell_type":"markdown","metadata":{},"source":["Code modified from https://github.com/Vaibhavs10/fast-whisper-finetuning"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare environment"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes==0.37 accelerate\n","%pip install -q git+https://github.com/huggingface/peft.git@main"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f740276b61704450a8f1eb80e753da95","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login() #huggingface-cli login workaround"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using the latest cached version of the module from /Users/amyzhou/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_13_0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055 (last modified on Mon Feb 26 15:50:02 2024) since it couldn't be found locally at mozilla-foundation/common_voice_13_0, or remotely on the Hugging Face Hub.\n","Using the latest cached version of the module from /Users/amyzhou/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_13_0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055 (last modified on Mon Feb 26 15:50:02 2024) since it couldn't be found locally at mozilla-foundation/common_voice_13_0, or remotely on the Hugging Face Hub.\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 6760\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 2947\n","    })\n","})\n"]}],"source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()\n","\n","language_abbr = \"hi\" # Replace with the language ID of your choice here!\n","\n","common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", language_abbr, split=\"train+validation\", use_auth_token=True)\n","common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", language_abbr, split=\"test\", use_auth_token=True)\n","\n","common_voice = common_voice.remove_columns(\n","    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\", \"variant\"]\n",")\n","\n","print(common_voice)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n","task = \"transcribe\"\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=language_abbr, task=task)\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=language_abbr, task=task)"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare data"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from datasets import Audio\n","\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n","# print(common_voice[\"train\"][0])"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch\n","\n","common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_features', 'labels'],\n","    num_rows: 6760\n","})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["common_voice[\"train\"]"]},{"cell_type":"markdown","metadata":{},"source":["# Training and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n","import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n","\n","data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric.\n","import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n","from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", load_in_8bit=True, device_map=\"auto\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Post-processing on model\n","from peft import prepare_model_for_int8_training\n","\n","model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")\n","\n","def make_inputs_require_grad(module, input, output):\n","    output.requires_grad_(True)\n","\n","model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply Low-rank adapters\n","from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n","\n","config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n","\n","model = get_peft_model(model, config)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define training configuration\n","from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-3,\n","    warmup_steps=50,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"steps\",\n","    fp16=True,\n","    per_device_eval_batch_size=8,\n","    generation_max_length=128,\n","    logging_steps=100,\n","#    max_steps=100, # only for testing purposes, remove this from your final run :)\n","    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n","    label_names=[\"labels\"],  # same reason as above\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","\n","# This callback helps to save only the adapter weights and remove the base model weights.\n","class SavePeftModelCallback(TrainerCallback):\n","    def on_save(\n","        self,\n","        args: TrainingArguments,\n","        state: TrainerState,\n","        control: TrainerControl,\n","        **kwargs,\n","    ):\n","        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n","\n","        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n","        kwargs[\"model\"].save_pretrained(peft_model_path)\n","\n","        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n","        if os.path.exists(pytorch_model_path):\n","            os.remove(pytorch_model_path)\n","        return control\n","\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    tokenizer=processor.feature_extractor,\n","    callbacks=[SavePeftModelCallback],\n",")\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Push to hub once done training\n","peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\"\n","model.push_to_hub(peft_model_id)"]},{"cell_type":"markdown","metadata":{},"source":["# Evalation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from peft import PeftModel, PeftConfig\n","from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n","\n","peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n","peft_config = PeftConfig.from_pretrained(peft_model_id)\n","model = WhisperForConditionalGeneration.from_pretrained(\n","    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",")\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","model.config.use_cache = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gc\n","import numpy as np\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n","\n","eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n","normalizer = BasicTextNormalizer()\n","\n","predictions = []\n","references = []\n","normalized_predictions = []\n","normalized_references = []\n","\n","model.eval()\n","for step, batch in enumerate(tqdm(eval_dataloader)):\n","    with torch.cuda.amp.autocast():\n","        with torch.no_grad():\n","            generated_tokens = (\n","                model.generate(\n","                    input_features=batch[\"input_features\"].to(\"cuda\"),\n","                    forced_decoder_ids=forced_decoder_ids,\n","                    max_new_tokens=255,\n","                )\n","                .cpu()\n","                .numpy()\n","            )\n","            labels = batch[\"labels\"].cpu().numpy()\n","            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n","            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","            predictions.extend(decoded_preds)\n","            references.extend(decoded_labels)\n","            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n","            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n","        del generated_tokens, labels, batch\n","    gc.collect()\n","wer = 100 * metric.compute(predictions=predictions, references=references)\n","normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n","eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n","\n","print(f\"{wer=} and {normalized_wer=}\")\n","print(eval_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import (\n","    AutomaticSpeechRecognitionPipeline,\n","    WhisperForConditionalGeneration,\n","    WhisperTokenizer,\n","    WhisperProcessor,\n",")\n","from peft import PeftModel, PeftConfig\n","\n","\n","peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n","language = \"hi\"\n","task = \"transcribe\"\n","peft_config = PeftConfig.from_pretrained(peft_model_id)\n","model = WhisperForConditionalGeneration.from_pretrained(\n","    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",")\n","\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n","processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n","feature_extractor = processor.feature_extractor\n","forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n","pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n","\n","\n","def transcribe(audio):\n","    with torch.cuda.amp.autocast():\n","        text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n","    return text\n","\n","transcribe(\"test_file.mp3\")"]}],"metadata":{"kernelspec":{"display_name":"local_nmt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":2}
