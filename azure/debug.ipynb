{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Setup \n",
    "os.system(\"pip install -q transformers librosa datasets==2.14.6 evaluate jiwer gradio bitsandbytes==0.37 accelerate geomloss gradio torchaudio\")\n",
    "os.system(\"pip install -q git+https://github.com/huggingface/peft.git@main\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (WhisperFeatureExtractor, \n",
    "                          WhisperTokenizer, \n",
    "                          WhisperProcessor,\n",
    "                          WhisperModel,\n",
    "                          WhisperForConditionalGeneration, \n",
    "                          Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, \n",
    "                          TrainerCallback, \n",
    "                          TrainingArguments, \n",
    "                          TrainerState, \n",
    "                          TrainerControl)\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from peft import (prepare_model_for_int8_training,\n",
    "                  LoraConfig, \n",
    "                  PeftModel, \n",
    "                  LoraModel, \n",
    "                  LoraConfig, \n",
    "                  TaskType,\n",
    "                  get_peft_model)\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.utils import check_min_version\n",
    "import re\n",
    "\n",
    "from trainer_utils import AlignmentSeq2SeqTrainer\n",
    "from data_utils import (DataCollatorSpeechSeq2SeqWithPadding, \n",
    "                        load_sd_qa_dataset, \n",
    "                        filter_data)\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # use first gpu on machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load whisper feature extractor, tokenizer, processor\n",
    "model_path = \"openai/whisper-base\"\n",
    "task = \"transcribe\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_path, task=task)\n",
    "\n",
    "    # load pre-trained model checkpoint\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    # model.hf_device_map = {\" \":0}  # not super sure what to map to here\n",
    "model.config.forced_decoder_ids = None  # no tokens forced for decoder outputs\n",
    "model.config.suppress_tokens = []\n",
    "model = model.to(device)\n",
    "    \n",
    "    # load data\n",
    "target_dialect = 'usa'\n",
    "source_dialect = 'ind_n'\n",
    "sd_qa = filter_data(load_sd_qa_dataset(), source=source_dialect, target=target_dialect)\n",
    "    \n",
    "print(sd_qa['dev'][0])\n",
    "\n",
    "    # prepare data\n",
    "def prepare_source_data(data):\n",
    "        # compute log-Mel input features from audio arrays\n",
    "        data[\"source_input_features\"] = feature_extractor(data[source_dialect][\"array\"], sampling_rate=data[source_dialect][\"sampling_rate\"]).input_features[0]\n",
    "        data[\"target_input_features\"] = feature_extractor(data[target_dialect][\"array\"], sampling_rate=data[target_dialect][\"sampling_rate\"]).input_features[0]\n",
    "        return data\n",
    "\n",
    "    # move to gpu\n",
    "    # run everything at once -> no for loop\n",
    "def prepare_target_embeddings(data):\n",
    "        # compute log-Mel input features from target audio array\n",
    "        # batch_size = 128\n",
    "        # target_embeddings = []\n",
    "        decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "        # for i in range(0, len(data[\"target_input_features\"]), batch_size):\n",
    "        input_features = torch.tensor(data[\"target_input_features\"])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_features, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.encoder_hidden_states[-1]\n",
    "        target_embeddings = [embedding for embedding in last_hidden_state]\n",
    "        data[\"target_embeddings\"] = target_embeddings\n",
    "        return data\n",
    "sample = sd_qa['dev'].select([10,11,12,13,14,15])\n",
    "sample = sample.map(prepare_source_data, num_proc=2, desc=\"Extract features for source dialect\"\n",
    "                      ).map(prepare_target_embeddings, desc=\"Original hidden embeddings for target dialect\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
