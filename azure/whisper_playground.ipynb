{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment\n",
    "Load libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers librosa datasets==2.14.6 evaluate jiwer gradio bitsandbytes==0.37 accelerate geomloss gradio torchaudio\n",
    "%pip install -q git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login() #huggingface-cli login workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (WhisperFeatureExtractor, \n",
    "                          WhisperTokenizer, \n",
    "                          WhisperProcessor,\n",
    "                          WhisperModel,\n",
    "                          WhisperForConditionalGeneration, \n",
    "                          Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, \n",
    "                          TrainerCallback, \n",
    "                          TrainingArguments, \n",
    "                          TrainerState, \n",
    "                          TrainerControl)\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from geomloss import SamplesLoss\n",
    "from peft import (prepare_model_for_int8_training,\n",
    "                  LoraConfig, \n",
    "                  PeftModel, \n",
    "                  LoraModel, \n",
    "                  LoraConfig, \n",
    "                  TaskType,\n",
    "                  get_peft_model)\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_qa = DatasetDict()\n",
    "\n",
    "sd_qa[\"dev\"] = load_dataset(\"WillHeld/SD-QA\", split=\"dev\", token=True)\n",
    "sd_qa[\"test\"] = load_dataset(\"WillHeld/SD-QA\", split=\"test\", token=True)\n",
    "saved_data = sd_qa\n",
    "print(sd_qa)\n",
    "print(sd_qa['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only target and source dialect\n",
    "sd_qa = saved_data\n",
    "target_dialect = 'usa'\n",
    "source_dialect = 'ind_n'\n",
    "sd_qa = sd_qa.select_columns(['id', \n",
    "                              source_dialect, \n",
    "                              target_dialect, \n",
    "                              # 'question'\n",
    "                              ])\n",
    "print(sd_qa['dev'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load whisper feature extractor, tokenizer, processor\n",
    "model_path = \"openai/whisper-base\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "task = \"transcribe\"\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_path, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prepare dataset function to extract audio features\n",
    "def prepare_source_data(data):\n",
    "    # compute log-Mel input features from audio arrays\n",
    "    data[\"source_input_features\"] = feature_extractor(data[source_dialect][\"array\"], sampling_rate=data[source_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    data[\"target_input_features\"] = feature_extractor(data[target_dialect][\"array\"], sampling_rate=data[target_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['target_embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fixed embeddings\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "def prepare_target_embeddings(data):\n",
    "    # compute log-Mel input features from target audio array\n",
    "    batch_size = 128\n",
    "    target_embeddings = []\n",
    "    decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "    for i in range(0, len(data[\"target_input_features\"]), batch_size):\n",
    "        input_features = torch.tensor(data[\"target_input_features\"][i: i + batch_size])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_features, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.encoder_hidden_states[-1]\n",
    "        target_embeddings.extend([embedding for embedding in last_hidden_state])\n",
    "    data[\"target_embeddings\"] = target_embeddings\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sd_qa['dev'].select([10,11,12,13,14,15])\n",
    "sample = sample.map(\n",
    "        prepare_source_data, \n",
    "        num_proc=2, \n",
    "        desc=\"Extract features for source dialect\"\n",
    "    )\n",
    "# .map(\n",
    "#         prepare_target_embeddings,\n",
    "#         batched=True,\n",
    "#         desc=\"Original hidden embeddings for target dialect\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample['target_embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map feature extracter to sd_qa\n",
    "# sd_qa = sd_qa.map(prepare_dataset, remove_columns=sd_qa.column_names[\"dev\"], num_proc=2)\n",
    "sd_qa = sd_qa.map(\n",
    "        prepare_source_data, \n",
    "        num_proc=2, \n",
    "        desc=\"Extract features for source dialect\"\n",
    "    ).map(\n",
    "        prepare_target_embeddings,\n",
    "        batched=True,\n",
    "        desc=\"Original hidden embeddings for target dialect\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, data: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # convert source inputs to pytorch tensors\n",
    "        input_features = {\"input_features\": data[\"source_input_features\"]} \n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        target_embeddings = torch.stack([torch.tensor(embedding) for embedding in data[\"target_embeddings\"]])\n",
    "        batch[\"target_embeddings\"] = target_embeddings\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize a data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample[\"source_input_features\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated = data_collator(sample)\n",
    "collated[\"input_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated[\"target_embeddings\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(sd_qa['dev'], batch_size=1)\n",
    "for batch in data_loader:\n",
    "    sample = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batched_data = data_collator(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2)\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    loss = sinkhorn_loss(pred.predictions, pred.target)\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [saved_data['dev'][0]['aus']['array'], saved_data['dev'][0]['usa']['array']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperForConditionalGeneration(\n",
      "  (model): WhisperModel(\n",
      "    (encoder): WhisperEncoder(\n",
      "      (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "      (embed_positions): Embedding(1500, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x WhisperEncoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): WhisperDecoder(\n",
      "      (embed_tokens): Embedding(51865, 512, padding_idx=50257)\n",
      "      (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x WhisperDecoderLayer(\n",
      "          (self_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): WhisperSdpaAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proj_out): Linear(in_features=512, out_features=51865, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing around and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of generating last encoder hidden state\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "example = [saved_data['dev'][0]['aus']['array'], saved_data['dev'][0]['usa']['array']]\n",
    "example = processor(example, return_tensors=\"pt\", sampling_rate=16000)\n",
    "decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(example.input_features, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "\n",
    "last_hidden_state = outputs.encoder_hidden_states[-1]\n",
    "# print(last_hidden_state)\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2)\n",
    "loss = sinkhorn_loss(last_hidden_state[0], last_hidden_state[1])\n",
    "print(loss)\n",
    "\n",
    "test = [embedding.flatten() for embedding in last_hidden_state]\n",
    "# print(test)\n",
    "\n",
    "loss = sinkhorn_loss(test[0], test[1])\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   outputs = model.generate(example.input_features, output_hidden_states=True)\n",
    "\n",
    "# decoded=processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tensors = torch.split(last_hidden_state, 1, dim=0)\n",
    "split_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path,load_in_8bit=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained checkpoint in 8b\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path,load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "# Post-processing steps on the model\n",
    "model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")\n",
    "\n",
    "# # Make inputs trainable\n",
    "# def make_inputs_require_grad(module, input, output):\n",
    "#     output.requires_grad_(True)\n",
    "\n",
    "# model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "def get_target_modules(model):\n",
    "    model_modules = str(model.modules)\n",
    "    pattern = r'\\((\\w+)\\): Linear'\n",
    "    linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "    names = []\n",
    "    # Print the names of the Linear layers\n",
    "    for name in linear_layer_names:\n",
    "        names.append(name)\n",
    "    target_modules = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model, targeting all layers of encoder\n",
    "target_modules = ['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2']\n",
    "config = LoraConfig(r=32, # rank, adjust this\n",
    "                    lora_alpha=64, \n",
    "                    target_modules = target_modules, \n",
    "                    lora_dropout=0.05, \n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "                    ) # task_type= ????? \n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Define training configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "#    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional PEFT things\n",
    "# This callback helps to save only the adapter weights and remove the base model weights.\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=sd_qa[\"dev\"],\n",
    "    eval_dataset=sd_qa[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to hub\n",
    "peft_model_id = \"azure-224n/whisper-base-100steps\"\n",
    "model.push_to_hub(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"openai/whisper-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n",
    "    [\"state\", \"text\"],\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = [39.793, 28.807, 25.512, 29.883]\n",
    "training_epochs = [0.8, 1.6, 2.4, 3]\n",
    "plt.plot(training_epochs, training_loss)\n",
    "plt.xlabel(\"training epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"loss over training epochs\")\n",
    "plt.savefig(\"loss_output_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
