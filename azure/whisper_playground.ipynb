{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment\n",
    "Load libraries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers librosa datasets==2.14.6 evaluate jiwer gradio bitsandbytes==0.37 accelerate geomloss gradio torchaudio\n",
    "%pip install -q git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login() #huggingface-cli login workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (WhisperFeatureExtractor, \n",
    "                          WhisperTokenizer, \n",
    "                          WhisperProcessor,\n",
    "                          WhisperModel,\n",
    "                          WhisperForConditionalGeneration, \n",
    "                          Seq2SeqTrainingArguments, \n",
    "                          Seq2SeqTrainer, \n",
    "                          TrainerCallback, \n",
    "                          TrainingArguments, \n",
    "                          TrainerState, \n",
    "                          TrainerControl)\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from geomloss import SamplesLoss\n",
    "from peft import (prepare_model_for_int8_training,\n",
    "                  LoraConfig, \n",
    "                  PeftModel, \n",
    "                  LoraModel, \n",
    "                  LoraConfig, \n",
    "                  TaskType,\n",
    "                  get_peft_model)\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "sd_qa = DatasetDict()\n",
    "\n",
    "sd_qa[\"dev\"] = load_dataset(\"WillHeld/SD-QA\", split=\"dev\", token=True)\n",
    "sd_qa[\"test\"] = load_dataset(\"WillHeld/SD-QA\", split=\"test\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'aus', 'gbr', 'ind_n', 'ind_s', 'irl', 'kenya', 'nga', 'nzl', 'phl', 'usa', 'zaf', 'answers', 'question'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'aus', 'gbr', 'ind_n', 'ind_s', 'irl', 'kenya', 'nga', 'nzl', 'phl', 'usa', 'zaf', 'answers', 'question'],\n",
      "        num_rows: 1031\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'aus', 'gbr', 'ind_n', 'ind_s', 'irl', 'kenya', 'nga', 'nzl', 'phl', 'usa', 'zaf', 'answers', 'question'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "saved_data = sd_qa\n",
    "print(sd_qa)\n",
    "print(sd_qa['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '-1008642825401516622', 'ind_n': {'path': None, 'array': array([ 0.00000000e+00, -3.05175781e-05, -3.05175781e-05, ...,\n",
      "        3.96728516e-04,  2.13623047e-04,  6.10351562e-05]), 'sampling_rate': 16000}, 'usa': {'path': None, 'array': array([0.        , 0.        , 0.        , ..., 0.00201416, 0.00259399,\n",
      "       0.00262451]), 'sampling_rate': 16000}, 'question': None}\n"
     ]
    }
   ],
   "source": [
    "# select only target and source dialect\n",
    "target_dialect = 'usa'\n",
    "source_dialect = 'ind_n'\n",
    "sd_qa = sd_qa.select_columns(['id', source_dialect, target_dialect, 'question'])\n",
    "print(sd_qa['dev'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load whisper feature extractor, tokenizer, processor\n",
    "model_path = \"openai/whisper-base\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "task = \"transcribe\"\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_path, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prepare dataset function to extract audio features\n",
    "def prepare_dataset(batch):\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"source_input_features\"] = feature_extractor(batch[source_dialect][\"array\"], sampling_rate=batch[source_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"target_input_features\"] = feature_extractor(batch[target_dialect][\"array\"], sampling_rate=batch[target_dialect][\"sampling_rate\"]).input_features[0]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a526f4444f7d4703a0878682bdc6d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad7ab2008f14dda9ea30e1959bf9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Applications/miniconda3/envs/cs224n-project-env/lib/python3.9/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# map feature extracter to sd_qa\n",
    "# sd_qa = sd_qa.map(prepare_dataset, remove_columns=sd_qa.column_names[\"dev\"], num_proc=2)\n",
    "sd_qa = sd_qa.map(prepare_dataset, remove_columns=[source_dialect, target_dialect], num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_qa['dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2Seq:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # convert source inputs to pytorch tensors\n",
    "        source_input_features = [{\"source_input_features\": feature[\"source_input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(source_input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # convert target inputs to pytorch tensors\n",
    "        target_input_features = [{\"target_input_features\": feature[\"target_input_features\"]} for feature in features]\n",
    "        batch[\"target_input_features\"] = self.processor.feature_extractor.pad(target_input_features, return_tensors=\"pt\")[\"target_input_features\"]\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Initialize a data collator\n",
    "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'source_input_features', 'target_input_features'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_qa['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2)\n",
    "\n",
    "# Define a function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    loss = sinkhorn_loss(pred.predictions, pred.target)\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [saved_data['dev'][0]['aus']['array'], saved_data['dev'][0]['usa']['array']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing around and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_qa['dev'][0]['ind_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of generating last encoder hidden state\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "example = [saved_data['dev'][0]['aus']['array'], saved_data['dev'][0]['usa']['array']]\n",
    "example = feature_extractor(example, return_tensors=\"pt\", sampling_rate=16000)\n",
    "decoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(example.input_features, decoder_input_ids=decoder_input_ids, output_hidden_states=True)\n",
    "\n",
    "last_hidden_state = outputs.encoder_hidden_states[-1]\n",
    "print(last_hidden_state)\n",
    "\n",
    "# loss = sinkhorn_loss(last_hidden_state[0], last_hidden_state[1])\n",
    "# print(loss)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   outputs = model.generate(example.input_features, output_hidden_states=True)\n",
    "\n",
    "# decoded=processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path,load_in_8bit=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained checkpoint in 8b\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path,load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "# Post-processing steps on the model\n",
    "model = prepare_model_for_int8_training(model, output_embedding_layer_name=\"proj_out\")\n",
    "\n",
    "# # Make inputs trainable\n",
    "# def make_inputs_require_grad(module, input, output):\n",
    "#     output.requires_grad_(True)\n",
    "\n",
    "# model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "def get_target_modules(model):\n",
    "    model_modules = str(model.modules)\n",
    "    pattern = r'\\((\\w+)\\): Linear'\n",
    "    linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "    names = []\n",
    "    # Print the names of the Linear layers\n",
    "    for name in linear_layer_names:\n",
    "        names.append(name)\n",
    "    target_modules = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to model, targeting all layers of encoder\n",
    "target_modules = ['k_proj', 'v_proj', 'q_proj', 'out_proj', 'fc1', 'fc2']\n",
    "config = LoraConfig(r=32, # rank, adjust this\n",
    "                    lora_alpha=64, \n",
    "                    target_modules = target_modules, \n",
    "                    lora_dropout=0.05, \n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "                    ) # task_type= ????? \n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Define training configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=100,\n",
    "#    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional PEFT things\n",
    "# This callback helps to save only the adapter weights and remove the base model weights.\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=sd_qa[\"dev\"],\n",
    "    eval_dataset=sd_qa[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to hub\n",
    "peft_model_id = \"azure-224n/whisper-base-100steps\"\n",
    "model.push_to_hub(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"openai/whisper-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
    "\n",
    "def transcribe(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    transcribe,\n",
    "    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n",
    "    [\"state\", \"text\"],\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
